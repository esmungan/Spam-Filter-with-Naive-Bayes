{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMS Spam filter with Multinomial Naive Bayes Algorithm\n",
    "\n",
    "\n",
    "This project is done as a part of Conditional Probability class on DataQuest.\n",
    "\n",
    "* **Concepts learned:** Naive Bayes Algorithm, Pandas-Numpy usage, Impact of different data cleaning schemes on the model's outcome\n",
    "* **Main challenges:** Underflow of the probability values for long texts, Finding vectorized solutions, Understanding the difference between the results for different cleaning methods.\n",
    "\n",
    "The goal of this project is to determine whether an SMS message is spam or not with an accuracy greater than 80%.To achieve this purpose, a multinomial naive Bayes model is employed.\n",
    "\n",
    "The data used in this project is from [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). It contains English, real and non-enconded messages, labeled according being legitimate (ham) or spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "Let us take a look at the content of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re as regex\n",
    "from IPython.display import display \n",
    "\n",
    "data_DF = pd.read_csv('SMSSpamCollection',sep='\\t',names=['Label', 'SMS'])\n",
    "data_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n"
     ]
    }
   ],
   "source": [
    "labels = data_DF[\"Label\"]\n",
    "print(labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label    0\n",
       "SMS      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_DF.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     5572\n",
       "unique       2\n",
       "top        ham\n",
       "freq      4825\n",
       "Name: Label, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data is composed of 5572 entries labeled as either \"ham\" (legitimate) or \"spam\". \n",
    "* There are no null values in the dataset\n",
    "* The SMS texts contain grammar errors, numbers and punctuations. \n",
    "* Finally, 87% of the messages in the set are ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "\n",
    "Before we start developing the model, we will clean up the dataset and tokenize the words. We can do different levels of data cleaning which would eventually impact the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations, lower case all the content and tokenize the words \n",
    "def normalized (txt):\n",
    "    clean_txt = regex.sub('\\W',' ',txt).lower().split()   \n",
    "    return clean_txt\n",
    "\n",
    "#Remove punctuations and tokenize the words \n",
    "def keep_capitals (txt): \n",
    "    clean_txt = regex.sub('\\W',' ',txt).split() \n",
    "    return clean_txt\n",
    "\n",
    "#Remove punctuations and stop words, lower case all the content and tokenize the words \n",
    "def no_stop_words (txt):\n",
    "\n",
    "    clean_txt = regex.sub('\\W',' ',txt).lower().split() \n",
    "\n",
    "    file = open(\"stop_words_english.txt\")  \n",
    "    stopW_set = set(file.read().split())\n",
    "    file.close()\n",
    "\n",
    "    new_list = []\n",
    "    for each_word in clean_txt:\n",
    "        if each_word not in stopW_set:\n",
    "            new_list.append(each_word)\n",
    "    clean_txt = new_list\n",
    "\n",
    "    return clean_txt\n",
    "\n",
    "#Remove punctuations, lower case all the content, tokenize and stem the words\n",
    "def stemmed (txt):\n",
    "    \n",
    "    from nltk.stem.porter import PorterStemmer \n",
    "    porter = PorterStemmer()\n",
    "        \n",
    "    clean_txt = regex.sub('\\W',' ',txt).lower().split() \n",
    "\n",
    "    new_list = []\n",
    "    for each_word in clean_txt:\n",
    "        new_list.append(porter.stem(each_word))\n",
    "    clean_txt = new_list\n",
    "\n",
    "    return clean_txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the scope of the original project, I experimented with these different data cleaning methods and I reported the results in the last section. Based on those results, the normalized version gives us the best accuracy. Hence, we will showcase it here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>clean_SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, i, don, t, think, he, goes, to, usf, he,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>[freemsg, hey, there, darling, it, s, been, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>[as, per, your, request, melle, melle, oru, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>[winner, as, a, valued, network, customer, you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>[had, your, mobile, 11, months, or, more, u, r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "6   ham  Even my brother is not like to speak with me. ...   \n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...   \n",
       "8  spam  WINNER!! As a valued network customer you have...   \n",
       "9  spam  Had your mobile 11 months or more? U R entitle...   \n",
       "\n",
       "                                           clean_SMS  \n",
       "0  [go, until, jurong, point, crazy, available, o...  \n",
       "1                     [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...  \n",
       "4  [nah, i, don, t, think, he, goes, to, usf, he,...  \n",
       "5  [freemsg, hey, there, darling, it, s, been, 3,...  \n",
       "6  [even, my, brother, is, not, like, to, speak, ...  \n",
       "7  [as, per, your, request, melle, melle, oru, mi...  \n",
       "8  [winner, as, a, valued, network, customer, you...  \n",
       "9  [had, your, mobile, 11, months, or, more, u, r...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions =[normalized,keep_capitals,no_stop_words,stemmed] \n",
    "\n",
    "data_DF[\"clean_SMS\"]=data_DF[\"SMS\"].apply(functions[0])  #you can change the function to experiment\n",
    "data_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing Data\n",
    "Now that we have the clean data set, we can randomize and divide it into the training and test datasets with a 4:1 ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>clean_SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Welp apparently he retired</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Havent.</td>\n",
       "      <td>[havent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I forgot 2 ask ü all smth.. There's a card on ...</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  \\\n",
       "0   ham                       Yep, by the pretty sculpture   \n",
       "1   ham      Yes, princess. Are you going to make me moan?   \n",
       "2   ham                         Welp apparently he retired   \n",
       "3   ham                                            Havent.   \n",
       "4   ham  I forgot 2 ask ü all smth.. There's a card on ...   \n",
       "\n",
       "                                           clean_SMS  \n",
       "0                  [yep, by, the, pretty, sculpture]  \n",
       "1  [yes, princess, are, you, going, to, make, me,...  \n",
       "2                    [welp, apparently, he, retired]  \n",
       "3                                           [havent]  \n",
       "4  [i, forgot, 2, ask, ü, all, smth, there, s, a,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed =1\n",
    "\n",
    "rnd_data_DF = data_DF.sample(frac=1, replace=False, random_state=seed)\n",
    "\n",
    "divider = round(len(rnd_data_DF)*0.8)\n",
    "\n",
    "train_DF= rnd_data_DF[0:divider].copy().reset_index(drop=True)\n",
    "test_DF= rnd_data_DF[divider:len(rnd_data_DF)].copy().reset_index(drop=True)\n",
    "\n",
    "train_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3858\n",
       "spam     600\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label= train_DF[\"Label\"]\n",
    "train_label.value_counts(dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     967\n",
       "spam    147\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label = test_DF[\"Label\"]\n",
    "test_label.value_counts(dropna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sets have a distribution similar to the original dataset and are ready-to-go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Implementation\n",
    "\n",
    "#### The Algorithm\n",
    "As a part of the Naive Bayes Algorithm, we will calculate the probability of a new SMS message being spam given the words inside it (P(Spam|w1,w2,w3 ..)) and compare it to the probability of the same message being ham (P(Ham|w1,w2,w3 ..)). If probability of the message being spam is higher, the SMS is going to be categorized as spam and vice versa.\n",
    "\n",
    "The probabilites are supposed to be calculated as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  P(Spam|w_1,...,w_n) = \\frac{ P(Spam \\cap (w_1,...,w_n)) }{ P((w_1,...,w_n)) }\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  P(Ham|w_1,...,w_n) = \\frac{ P(Ham \\cap (w_1,...,w_n)) }{ P((w_1,...,w_n)) }\n",
    "\\end{equation}\n",
    "\n",
    "but we will make two simplifications: \n",
    "\n",
    "**1)** The first one is nott to calculate the denominator of the functions above. This is mainly because we will compare (P(Spam|w1,w2,..,wn)) to (P(Ham|w1,w2,..,wn)) and both will have the same denominator. Hence, the equation will be simplified to:\n",
    "\n",
    "\\begin{equation}\n",
    "  P(Spam|w_1,...,w_n) \\propto P(Spam \\cap (w_1,...,w_n))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  P(Ham|w_1,...,w_n) \\propto P(Ham \\cap (w_1,...,w_n))\n",
    "\\end{equation}\n",
    "\n",
    "    which eventually can be calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "  P(Spam \\cap (w_1,...,w_n)) = P(w1| w_2 \\cap ... \\cap w_n \\cap Spam)* \n",
    "                               P(w2| w_3 \\cap ... \\cap w_n \\cap Spam)* ...* P(wn|Spam)* P(Spam) \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  P(Ham \\cap (w_1,...,w_n)) = P(w1| w_2 \\cap ... \\cap w_n \\cap Ham)* \n",
    "                              P(w2| w_3 \\cap ... \\cap w_n \\cap Ham)* ...* P(wn|Ham)* P(Ham) \n",
    "\\end{equation}\n",
    "<br/> <br/>   \n",
    "\n",
    "**2)** The second simplification is to assume (naively) that the probabilities of the words occuring in a message is conditionally independent given the message is spam (or ham). That is to say, the equation can above can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "  P(Spam \\cap (w_1,...,w_n)) =  P(Spam) * P(w_1|Spam) * ... * P(w_n|Spam) \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  P(Ham \\cap (w_1,...,w_n)) =  P(Ham) * P(w_1|Ham) * ... * P(w_n|Ham) \n",
    "\\end{equation}\n",
    "\n",
    "    or simply:\n",
    "    \n",
    "\\begin{equation}\n",
    "  P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "    where (P(wi|Spam) can be calculated as: \n",
    "\n",
    "\\begin{equation}\n",
    "  P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "    Here:\n",
    "\n",
    "  - N<sub>wi |Spam</sub> (or N<sub>wi |Ham</sub>) is the number of times a word appears in spam (or ham) messages\n",
    "  - $\\alpha $ is the Laplace smoothing factor of 1\n",
    "  - N<sub>Spam</sub> (or N<sub>Ham</sub>) is the number of words in spam (or ham) messages\n",
    "  - N<sub>Vocabulary</sub> is the number of unique words in the training data set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Implementation\n",
    "\n",
    "Let us first create the vocabulary of the unique words in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '00', '000', '000pes', '008704050406', '0089', '01223585334', '02', '0207', '02072069400']\n",
      "['zindgi', 'zoe', 'zogtorius', 'zouk', 'zyada', 'é', 'ú1', 'ü', '〨ud', '鈥']\n"
     ]
    }
   ],
   "source": [
    "train_SMS = train_DF[\"clean_SMS\"]\n",
    "\n",
    "tokens =[]\n",
    "for each_row in train_SMS:\n",
    "    for each_word in each_row:\n",
    "        tokens.append(each_word)\n",
    "\n",
    "vocab_set = set(tokens)\n",
    "train_vocab = list(vocab_set)\n",
    "    \n",
    "train_vocab.sort()\n",
    "print(train_vocab[:10])\n",
    "print(train_vocab[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , we can caluclate low haning fruits for (P(wi|Spam)  calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_ham 0.865,p_spam 0.135\n",
      "n_ham 3.86e+03,n_spam 6.00e+02\n",
      "n_vocabulary 7783\n"
     ]
    }
   ],
   "source": [
    "ratios = train_label.value_counts(normalize=True)      \n",
    "p_ham = ratios[\"ham\"]\n",
    "p_spam = ratios[\"spam\"]\n",
    "\n",
    "counts = train_label.value_counts()\n",
    "n_ham = counts[\"ham\"]\n",
    "n_spam = counts[\"spam\"]\n",
    "\n",
    "n_vocab = len(train_vocab)\n",
    "alpha = 1\n",
    "\n",
    "print((\"p_ham %.3f,p_spam %.3f\\n\"+\"n_ham %.2e,n_spam %.2e\\n\"+\"n_vocabulary %i\") \n",
    "      %(p_ham,p_spam,n_ham,n_spam,n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to calculate is the number of times a word appears in spam (or ham) messages **:** N<sub>wi |Spam</sub> (or N<sub>wi |Ham</sub>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   00  000  000pes  008704050406  0089  01223585334   02  0207  \\\n",
       "Label                                                                      \n",
       "ham    0.0  0.0  0.0     0.0           0.0   0.0          0.0  0.0   0.0   \n",
       "ham    0.0  0.0  0.0     0.0           0.0   0.0          0.0  0.0   0.0   \n",
       "ham    0.0  0.0  0.0     0.0           0.0   0.0          0.0  0.0   0.0   \n",
       "ham    0.0  0.0  0.0     0.0           0.0   0.0          0.0  0.0   0.0   \n",
       "ham    0.0  0.0  0.0     0.0           0.0   0.0          0.0  0.0   0.0   \n",
       "\n",
       "       02072069400  ...  zindgi  zoe  zogtorius  zouk  zyada    é   ú1    ü  \\\n",
       "Label               ...                                                       \n",
       "ham            0.0  ...     0.0  0.0        0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "ham            0.0  ...     0.0  0.0        0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "ham            0.0  ...     0.0  0.0        0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "ham            0.0  ...     0.0  0.0        0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "ham            0.0  ...     0.0  0.0        0.0   0.0    0.0  0.0  0.0  2.0   \n",
       "\n",
       "       〨ud    鈥  \n",
       "Label            \n",
       "ham    0.0  0.0  \n",
       "ham    0.0  0.0  \n",
       "ham    0.0  0.0  \n",
       "ham    0.0  0.0  \n",
       "ham    0.0  0.0  \n",
       "\n",
       "[5 rows x 7783 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_freq={}\n",
    "for unique_word in train_vocab:\n",
    "    train_word_freq[unique_word] = np.zeros(len(train_SMS))\n",
    "\n",
    "for index,each_row in enumerate(train_SMS):\n",
    "    for each_word in each_row:\n",
    "        train_word_freq[each_word][index] += 1\n",
    "\n",
    "twf = pd.DataFrame(train_word_freq)\n",
    "\n",
    "train_freq_DF = pd.concat([train_label,twf],axis=1)  \n",
    "train_freq_DF.set_index('Label',inplace=True)\n",
    "train_freq_DF.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   00   000  000pes  008704050406  0089  01223585334   02  0207  \\\n",
       "Label                                                                       \n",
       "ham    0.0  0.0   0.0     1.0           0.0   0.0          0.0  0.0   0.0   \n",
       "spam   3.0  9.0  25.0     0.0           1.0   1.0          2.0  7.0   3.0   \n",
       "\n",
       "       02072069400  ...  zindgi  zoe  zogtorius  zouk  zyada    é   ú1      ü  \\\n",
       "Label               ...                                                         \n",
       "ham            0.0  ...     2.0  1.0        1.0   0.0    1.0  4.0  0.0  128.0   \n",
       "spam           1.0  ...     0.0  0.0        0.0   1.0    0.0  0.0  1.0    0.0   \n",
       "\n",
       "       〨ud    鈥  \n",
       "Label            \n",
       "ham    1.0  1.0  \n",
       "spam   0.0  0.0  \n",
       "\n",
       "[2 rows x 7783 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_DF = train_freq_DF.groupby(\"Label\").sum()\n",
    "display(tf_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last parameters we need are the number of words in spam (or ham) messages **:** N<sub>Spam</sub> (or N<sub>Ham</sub>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in ham messages: 57237 , spam messages: 15190\n"
     ]
    }
   ],
   "source": [
    "n_ham_words = tf_DF.loc[\"ham\"].sum()\n",
    "n_spam_words = tf_DF.loc[\"spam\"].sum()\n",
    "print(\"Number of words in ham messages: %i , spam messages: %i\" %(n_ham_words,n_spam_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all we need, we can calculate the P(wi|Spam) ans P(wi|Ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        00       000    000pes  008704050406      0089  \\\n",
       "Label                                                                   \n",
       "ham    0.000015  0.000015  0.000015  0.000031      0.000015  0.000015   \n",
       "spam   0.000174  0.000435  0.001132  0.000044      0.000087  0.000087   \n",
       "\n",
       "       01223585334        02      0207  02072069400  ...    zindgi       zoe  \\\n",
       "Label                                                ...                       \n",
       "ham       0.000015  0.000015  0.000015     0.000015  ...  0.000046  0.000031   \n",
       "spam      0.000131  0.000348  0.000174     0.000087  ...  0.000044  0.000044   \n",
       "\n",
       "       zogtorius      zouk     zyada         é        ú1         ü       〨ud  \\\n",
       "Label                                                                          \n",
       "ham     0.000031  0.000015  0.000031  0.000077  0.000015  0.001984  0.000031   \n",
       "spam    0.000044  0.000087  0.000044  0.000044  0.000087  0.000044  0.000044   \n",
       "\n",
       "              鈥  \n",
       "Label            \n",
       "ham    0.000031  \n",
       "spam   0.000044  \n",
       "\n",
       "[2 rows x 7783 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_div_by ={'ham': n_ham_words+(alpha*n_vocab),'spam' : n_spam_words+(alpha*n_vocab)}\n",
    "div_by = tf_DF.index.to_series().map(dict_div_by)\n",
    "\n",
    "p_word_DF= (tf_DF+alpha).div(div_by, axis = 0)\n",
    "display(p_word_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, we will: \n",
    "* Calculate the P(Spam|w1,...,wn) from P(wi|Spam) **and** P(Ham|w1,...,wn) from P(wi|Ham)\n",
    "* Assign the message as: \n",
    "    - spam      if P(Spam|w1,...,wn) > P(Ham|w1,...,wn)\n",
    "    - ham       if P(Spam|w1,...,wn) < P(Ham|w1,...,wn)\n",
    "    - ambigious if  P(Spam|w1,...,wn) == P(Ham|w1,...,wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_or_ham (txt,debug=False):\n",
    "    \n",
    "    if debug: \n",
    "        print('\\n#--------------------------------------------')\n",
    "        print('SMS:\\n',txt) \n",
    "    \n",
    "    txt_in_vocab = []\n",
    "    for word in txt:\n",
    "        if word in train_vocab:\n",
    "             txt_in_vocab.append(word)   \n",
    "\n",
    "    if debug: print('Words found in vocab:\\n',txt_in_vocab)\n",
    "\n",
    "    p_ham_given_words  = p_ham\n",
    "    p_spam_given_words = p_spam   \n",
    "    \n",
    "    if (txt_in_vocab):\n",
    "        p_ham_given_words  *= p_word_DF.loc[\"ham\",txt_in_vocab].product(skipna=False)\n",
    "        p_spam_given_words *= p_word_DF.loc[\"spam\",txt_in_vocab].product(skipna=False) \n",
    "   \n",
    "    if(p_ham_given_words>p_spam_given_words):                      \n",
    "        category = \"ham\"\n",
    "    elif (p_ham_given_words<p_spam_given_words):\n",
    "        category = \"spam\"\n",
    "    else:\n",
    "        category = \"ambigious\"\n",
    "    \n",
    "       \n",
    "    if debug:\n",
    "        print()\n",
    "        print('p_ham_prior: %.3e' %p_ham)\n",
    "        print('p_ham_given_words ~ %.3e' %p_ham_given_words)\n",
    "        print('Word','\\t','P(wi|ham)')\n",
    "        print(p_word_DF.loc[\"ham\",txt_in_vocab],'\\n')\n",
    "        print('p_spam_prior: %.3e' %p_spam)\n",
    "        print('p_spam_given_words ~ %.3e' %p_spam_given_words)\n",
    "        print('Word','\\t','P(wi|spam)')\n",
    "        print(p_word_DF.loc[\"spam\",txt_in_vocab])\n",
    "        \n",
    "    return category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the function above for the first entry in the test_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['later', 'i', 'guess', 'i', 'needa', 'do', 'mcat', 'study', 'too']\n",
      "Words found in vocab:\n",
      " ['later', 'i', 'guess', 'i', 'do', 'study', 'too']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 4.253e-19\n",
      "Word \t P(wi|ham)\n",
      "later    0.001523\n",
      "i        0.036942\n",
      "guess    0.000323\n",
      "i        0.036942\n",
      "do       0.004860\n",
      "study    0.000108\n",
      "too      0.001400\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 3.483e-26\n",
      "Word \t P(wi|spam)\n",
      "later    0.000044\n",
      "i        0.002220\n",
      "guess    0.000305\n",
      "i        0.002220\n",
      "do       0.001045\n",
      "study    0.000044\n",
      "too      0.000087\n",
      "Name: spam, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_or_ham(test_DF.loc[0,'clean_SMS'],debug=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SMS seems to have been parsed and filtered correctly. The parameters for the calculations also look right.\n",
    "\n",
    "Now, it is time to apply the function to the whole test dataset and get the accuracy of the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF['Prediction']=test_DF['clean_SMS'].apply(spam_or_ham, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>clean_SMS</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>[later, i, guess, i, needa, do, mcat, study, too]</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>[but, i, haf, enuff, space, got, like, 4, mb]</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>[had, your, mobile, 10, mths, update, to, late...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>[all, sounds, good, fingers, makes, it, diffic...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>[all, done, all, handed, in, don, t, know, if,...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  \\\n",
       "0   ham          Later i guess. I needa do mcat study too.   \n",
       "1   ham             But i haf enuff space got like 4 mb...   \n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...   \n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...   \n",
       "4   ham  All done, all handed in. Don't know if mega sh...   \n",
       "\n",
       "                                           clean_SMS Prediction  \n",
       "0  [later, i, guess, i, needa, do, mcat, study, too]        ham  \n",
       "1      [but, i, haf, enuff, space, got, like, 4, mb]        ham  \n",
       "2  [had, your, mobile, 10, mths, update, to, late...       spam  \n",
       "3  [all, sounds, good, fingers, makes, it, diffic...        ham  \n",
       "4  [all, done, all, handed, in, don, t, know, if,...        ham  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries: 1114\n",
      "Number of errors: 14\n",
      "Accuracy: 0.9874\n"
     ]
    }
   ],
   "source": [
    "check= (test_DF['Label']==test_DF['Prediction']).value_counts()\n",
    "correct = check[True]\n",
    "error = check[False]\n",
    "accuracy = correct/(correct+error)\n",
    "\n",
    "print(\"Total number of entries:\",correct+error)\n",
    "print(\"Number of errors:\",error)\n",
    "print(\"Accuracy: %.4f\" %accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check for which cases the model fails to predict correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>clean_SMS</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>spam</td>\n",
       "      <td>Not heard from U4 a while. Call me now am here...</td>\n",
       "      <td>[not, heard, from, u4, a, while, call, me, now...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>spam</td>\n",
       "      <td>More people are dogging in your area now. Call...</td>\n",
       "      <td>[more, people, are, dogging, in, your, area, n...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>ham</td>\n",
       "      <td>Unlimited texts. Limited minutes.</td>\n",
       "      <td>[unlimited, texts, limited, minutes]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>ham</td>\n",
       "      <td>26th OF JULY</td>\n",
       "      <td>[26th, of, july]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nokia phone is lovly..</td>\n",
       "      <td>[nokia, phone, is, lovly]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>ham</td>\n",
       "      <td>A Boy loved a gal. He propsd bt she didnt mind...</td>\n",
       "      <td>[a, boy, loved, a, gal, he, propsd, bt, she, d...</td>\n",
       "      <td>ambigious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>ham</td>\n",
       "      <td>No calls..messages..missed calls</td>\n",
       "      <td>[no, calls, messages, missed, calls]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>ham</td>\n",
       "      <td>We have sent JD for Customer Service cum Accou...</td>\n",
       "      <td>[we, have, sent, jd, for, customer, service, c...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>spam</td>\n",
       "      <td>Oh my god! I've found your number again! I'm s...</td>\n",
       "      <td>[oh, my, god, i, ve, found, your, number, agai...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hi babe its Chloe, how r u? I was smashed on s...</td>\n",
       "      <td>[hi, babe, its, chloe, how, r, u, i, was, smas...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>spam</td>\n",
       "      <td>0A$NETWORKS allow companies to bill for SMS, s...</td>\n",
       "      <td>[0a, networks, allow, companies, to, bill, for...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>spam</td>\n",
       "      <td>RCT' THNQ Adrian for U text. Rgds Vatian</td>\n",
       "      <td>[rct, thnq, adrian, for, u, text, rgds, vatian]</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>spam</td>\n",
       "      <td>2/2 146tf150p</td>\n",
       "      <td>[2, 2, 146tf150p]</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hello. We need some posh birds and chaps to us...</td>\n",
       "      <td>[hello, we, need, some, posh, birds, and, chap...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label                                                SMS  \\\n",
       "114  spam  Not heard from U4 a while. Call me now am here...   \n",
       "135  spam  More people are dogging in your area now. Call...   \n",
       "152   ham                  Unlimited texts. Limited minutes.   \n",
       "159   ham                                       26th OF JULY   \n",
       "284   ham                             Nokia phone is lovly..   \n",
       "293   ham  A Boy loved a gal. He propsd bt she didnt mind...   \n",
       "302   ham                   No calls..messages..missed calls   \n",
       "319   ham  We have sent JD for Customer Service cum Accou...   \n",
       "504  spam  Oh my god! I've found your number again! I'm s...   \n",
       "546  spam  Hi babe its Chloe, how r u? I was smashed on s...   \n",
       "741  spam  0A$NETWORKS allow companies to bill for SMS, s...   \n",
       "876  spam           RCT' THNQ Adrian for U text. Rgds Vatian   \n",
       "885  spam                                      2/2 146tf150p   \n",
       "953  spam  Hello. We need some posh birds and chaps to us...   \n",
       "\n",
       "                                             clean_SMS Prediction  \n",
       "114  [not, heard, from, u4, a, while, call, me, now...        ham  \n",
       "135  [more, people, are, dogging, in, your, area, n...        ham  \n",
       "152               [unlimited, texts, limited, minutes]       spam  \n",
       "159                                   [26th, of, july]       spam  \n",
       "284                          [nokia, phone, is, lovly]       spam  \n",
       "293  [a, boy, loved, a, gal, he, propsd, bt, she, d...  ambigious  \n",
       "302               [no, calls, messages, missed, calls]       spam  \n",
       "319  [we, have, sent, jd, for, customer, service, c...       spam  \n",
       "504  [oh, my, god, i, ve, found, your, number, agai...        ham  \n",
       "546  [hi, babe, its, chloe, how, r, u, i, was, smas...        ham  \n",
       "741  [0a, networks, allow, companies, to, bill, for...        ham  \n",
       "876    [rct, thnq, adrian, for, u, text, rgds, vatian]        ham  \n",
       "885                                  [2, 2, 146tf150p]        ham  \n",
       "953  [hello, we, need, some, posh, birds, and, chap...        ham  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_DF[test_DF['Prediction']!=test_DF['Label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight, the message categorized as ambigious looks troublesome. Let us dive deeper and see what went wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['not', 'heard', 'from', 'u4', 'a', 'while', 'call', 'me', 'now', 'am', 'here', 'all', 'night', 'with', 'just', 'my', 'knickers', 'on', 'make', 'me', 'beg', 'for', 'it', 'like', 'u', 'did', 'last', 'time', '01223585236', 'xx', 'luv', 'nikiyu4', 'net']\n",
      "Words found in vocab:\n",
      " ['not', 'heard', 'from', 'u4', 'a', 'while', 'call', 'me', 'now', 'am', 'here', 'all', 'night', 'with', 'just', 'my', 'on', 'make', 'me', 'beg', 'for', 'it', 'like', 'u', 'did', 'last', 'time', 'xx', 'luv', 'net']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 1.182e-84\n",
      "Word \t P(wi|ham)\n",
      "not      0.005291\n",
      "heard    0.000123\n",
      "from     0.001784\n",
      "u4       0.000015\n",
      "a        0.013396\n",
      "while    0.000308\n",
      "call     0.003045\n",
      "me       0.009843\n",
      "now      0.003768\n",
      "am       0.002599\n",
      "here     0.001476\n",
      "all      0.003107\n",
      "night    0.001292\n",
      "with     0.003230\n",
      "just     0.003676\n",
      "my       0.009197\n",
      "on       0.004783\n",
      "make     0.001184\n",
      "me       0.009843\n",
      "beg      0.000031\n",
      "for      0.006337\n",
      "it       0.009043\n",
      "like     0.002876\n",
      "u        0.012688\n",
      "did      0.001676\n",
      "last     0.000800\n",
      "time     0.002538\n",
      "xx       0.000169\n",
      "luv      0.000384\n",
      "net      0.000123\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 2.000e-94\n",
      "Word \t P(wi|spam)\n",
      "not      0.000871\n",
      "heard    0.000131\n",
      "from     0.004440\n",
      "u4       0.000131\n",
      "a        0.013407\n",
      "while    0.000131\n",
      "call     0.012624\n",
      "me       0.001088\n",
      "now      0.007400\n",
      "am       0.000522\n",
      "here     0.000218\n",
      "all      0.001219\n",
      "night    0.000174\n",
      "with     0.003874\n",
      "just     0.002786\n",
      "my       0.000392\n",
      "on       0.004919\n",
      "make     0.000479\n",
      "me       0.001088\n",
      "beg      0.000044\n",
      "for      0.006704\n",
      "it       0.001132\n",
      "like     0.000479\n",
      "u        0.005659\n",
      "did      0.000087\n",
      "last     0.000522\n",
      "time     0.000740\n",
      "xx       0.000131\n",
      "luv      0.000218\n",
      "net      0.000566\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['more', 'people', 'are', 'dogging', 'in', 'your', 'area', 'now', 'call', '09090204448', 'and', 'join', 'like', 'minded', 'guys', 'why', 'not', 'arrange', '1', 'yourself', 'there', 's', '1', 'this', 'evening', 'a', '1', '50', 'minapn', 'ls278bb']\n",
      "Words found in vocab:\n",
      " ['more', 'people', 'are', 'dogging', 'in', 'your', 'area', 'now', 'call', 'and', 'join', 'like', 'guys', 'why', 'not', 'arrange', '1', 'yourself', 'there', 's', '1', 'this', 'evening', 'a', '1', '50']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 2.035e-78\n",
      "Word \t P(wi|ham)\n",
      "more        0.000969\n",
      "people      0.000584\n",
      "are         0.005198\n",
      "dogging     0.000015\n",
      "in          0.010228\n",
      "your        0.005229\n",
      "area        0.000077\n",
      "now         0.003768\n",
      "call        0.003045\n",
      "and         0.010551\n",
      "join        0.000138\n",
      "like        0.002876\n",
      "guys        0.000431\n",
      "why         0.000861\n",
      "not         0.005291\n",
      "arrange     0.000046\n",
      "1           0.000800\n",
      "yourself    0.000261\n",
      "there       0.002676\n",
      "s           0.005967\n",
      "1           0.000800\n",
      "this        0.003168\n",
      "evening     0.000415\n",
      "a           0.013396\n",
      "1           0.000800\n",
      "50          0.000077\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 1.004e-78\n",
      "Word \t P(wi|spam)\n",
      "more        0.000871\n",
      "people      0.000131\n",
      "are         0.002829\n",
      "dogging     0.000218\n",
      "in          0.002960\n",
      "your        0.009228\n",
      "area        0.000305\n",
      "now         0.007400\n",
      "call        0.012624\n",
      "and         0.004266\n",
      "join        0.000479\n",
      "like        0.000479\n",
      "guys        0.000261\n",
      "why         0.000479\n",
      "not         0.000871\n",
      "arrange     0.000087\n",
      "1           0.003961\n",
      "yourself    0.000044\n",
      "there       0.000609\n",
      "s           0.002786\n",
      "1           0.003961\n",
      "this        0.003091\n",
      "evening     0.000044\n",
      "a           0.013407\n",
      "1           0.003961\n",
      "50          0.001785\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['unlimited', 'texts', 'limited', 'minutes']\n",
      "Words found in vocab:\n",
      " ['unlimited', 'texts', 'minutes']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 1.385e-12\n",
      "Word \t P(wi|ham)\n",
      "unlimited    0.000062\n",
      "texts        0.000077\n",
      "minutes      0.000338\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 1.055e-11\n",
      "Word \t P(wi|spam)\n",
      "unlimited    0.000435\n",
      "texts        0.000827\n",
      "minutes      0.000218\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['26th', 'of', 'july']\n",
      "Words found in vocab:\n",
      " ['26th', 'of', 'july']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 1.319e-12\n",
      "Word \t P(wi|ham)\n",
      "26th    0.000015\n",
      "of      0.006444\n",
      "july    0.000015\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 5.328e-12\n",
      "Word \t P(wi|spam)\n",
      "26th    0.000087\n",
      "of      0.003482\n",
      "july    0.000131\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['nokia', 'phone', 'is', 'lovly']\n",
      "Words found in vocab:\n",
      " ['nokia', 'phone', 'is']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 3.342e-10\n",
      "Word \t P(wi|ham)\n",
      "nokia    0.000046\n",
      "phone    0.000938\n",
      "is       0.008920\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 3.230e-09\n",
      "Word \t P(wi|spam)\n",
      "nokia    0.002568\n",
      "phone    0.001567\n",
      "is       0.005964\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['a', 'boy', 'loved', 'a', 'gal', 'he', 'propsd', 'bt', 'she', 'didnt', 'mind', 'he', 'gv', 'lv', 'lttrs', 'bt', 'her', 'frnds', 'threw', 'thm', 'again', 'd', 'boy', 'decided', '2', 'aproach', 'd', 'gal', 'dt', 'time', 'a', 'truck', 'was', 'speeding', 'towards', 'd', 'gal', 'wn', 'it', 'was', 'about', '2', 'hit', 'd', 'girl', 'd', 'boy', 'ran', 'like', 'hell', 'n', 'saved', 'her', 'she', 'asked', 'hw', 'cn', 'u', 'run', 'so', 'fast', 'd', 'boy', 'replied', 'boost', 'is', 'd', 'secret', 'of', 'my', 'energy', 'n', 'instantly', 'd', 'girl', 'shouted', 'our', 'energy', 'n', 'thy', 'lived', 'happily', '2gthr', 'drinking', 'boost', 'evrydy', 'moral', 'of', 'd', 'story', 'i', 'hv', 'free', 'msgs', 'd', 'gud', 'ni8']\n",
      "Words found in vocab:\n",
      " ['a', 'boy', 'loved', 'a', 'gal', 'he', 'propsd', 'bt', 'she', 'didnt', 'mind', 'he', 'gv', 'lv', 'lttrs', 'bt', 'her', 'frnds', 'threw', 'thm', 'again', 'd', 'boy', 'decided', '2', 'aproach', 'd', 'gal', 'dt', 'time', 'a', 'truck', 'was', 'speeding', 'towards', 'd', 'gal', 'wn', 'it', 'was', 'about', '2', 'hit', 'd', 'girl', 'd', 'boy', 'ran', 'like', 'hell', 'n', 'saved', 'her', 'she', 'asked', 'hw', 'cn', 'u', 'run', 'so', 'fast', 'd', 'boy', 'replied', 'boost', 'is', 'd', 'secret', 'of', 'my', 'energy', 'n', 'instantly', 'd', 'girl', 'shouted', 'our', 'energy', 'n', 'thy', 'lived', 'happily', '2gthr', 'drinking', 'boost', 'evrydy', 'moral', 'of', 'd', 'story', 'i', 'hv', 'free', 'msgs', 'd', 'gud', 'ni8']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 0.000e+00\n",
      "Word \t P(wi|ham)\n",
      "a            0.013396\n",
      "boy          0.000338\n",
      "loved        0.000138\n",
      "a            0.013396\n",
      "gal          0.000154\n",
      "he           0.002861\n",
      "propsd       0.000031\n",
      "bt           0.000277\n",
      "she          0.002122\n",
      "didnt        0.000384\n",
      "mind         0.000523\n",
      "he           0.002861\n",
      "gv           0.000031\n",
      "lv           0.000031\n",
      "lttrs        0.000031\n",
      "bt           0.000277\n",
      "her          0.001538\n",
      "frnds        0.000154\n",
      "threw        0.000031\n",
      "thm          0.000046\n",
      "again        0.000661\n",
      "d            0.001615\n",
      "boy          0.000338\n",
      "decided      0.000215\n",
      "2            0.004045\n",
      "aproach      0.000031\n",
      "d            0.001615\n",
      "gal          0.000154\n",
      "dt           0.000031\n",
      "time         0.002538\n",
      "               ...   \n",
      "secret       0.000062\n",
      "of           0.006444\n",
      "my           0.009197\n",
      "energy       0.000062\n",
      "n            0.001738\n",
      "instantly    0.000031\n",
      "d            0.001615\n",
      "girl         0.000308\n",
      "shouted      0.000046\n",
      "our          0.000800\n",
      "energy       0.000062\n",
      "n            0.001738\n",
      "thy          0.000031\n",
      "lived        0.000031\n",
      "happily      0.000031\n",
      "2gthr        0.000031\n",
      "drinking     0.000046\n",
      "boost        0.000046\n",
      "evrydy       0.000031\n",
      "moral        0.000108\n",
      "of           0.006444\n",
      "d            0.001615\n",
      "story        0.000277\n",
      "i            0.036942\n",
      "hv           0.000062\n",
      "free         0.000754\n",
      "msgs         0.000123\n",
      "d            0.001615\n",
      "gud          0.000831\n",
      "ni8          0.000200\n",
      "Name: ham, Length: 97, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 0.000e+00\n",
      "Word \t P(wi|spam)\n",
      "a            0.013407\n",
      "boy          0.000044\n",
      "loved        0.000044\n",
      "a            0.013407\n",
      "gal          0.000044\n",
      "he           0.000044\n",
      "propsd       0.000044\n",
      "bt           0.000653\n",
      "she          0.000044\n",
      "didnt        0.000044\n",
      "mind         0.000087\n",
      "he           0.000044\n",
      "gv           0.000044\n",
      "lv           0.000044\n",
      "lttrs        0.000044\n",
      "bt           0.000653\n",
      "her          0.000435\n",
      "frnds        0.000044\n",
      "threw        0.000044\n",
      "thm          0.000044\n",
      "again        0.000131\n",
      "d            0.000305\n",
      "boy          0.000044\n",
      "decided      0.000044\n",
      "2            0.007226\n",
      "aproach      0.000044\n",
      "d            0.000305\n",
      "gal          0.000044\n",
      "dt           0.000044\n",
      "time         0.000740\n",
      "               ...   \n",
      "secret       0.000348\n",
      "of           0.003482\n",
      "my           0.000392\n",
      "energy       0.000131\n",
      "n            0.000392\n",
      "instantly    0.000087\n",
      "d            0.000305\n",
      "girl         0.000174\n",
      "shouted      0.000044\n",
      "our          0.002916\n",
      "energy       0.000131\n",
      "n            0.000392\n",
      "thy          0.000044\n",
      "lived        0.000044\n",
      "happily      0.000044\n",
      "2gthr        0.000044\n",
      "drinking     0.000044\n",
      "boost        0.000044\n",
      "evrydy       0.000044\n",
      "moral        0.000044\n",
      "of           0.003482\n",
      "d            0.000305\n",
      "story        0.000044\n",
      "i            0.002220\n",
      "hv           0.000044\n",
      "free         0.007661\n",
      "msgs         0.000305\n",
      "d            0.000305\n",
      "gud          0.000044\n",
      "ni8          0.000044\n",
      "Name: spam, Length: 97, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['no', 'calls', 'messages', 'missed', 'calls']\n",
      "Words found in vocab:\n",
      " ['no', 'calls', 'messages', 'missed', 'calls']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 3.031e-18\n",
      "Word \t P(wi|ham)\n",
      "no          0.003660\n",
      "calls       0.000154\n",
      "messages    0.000138\n",
      "missed      0.000292\n",
      "calls       0.000154\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 9.486e-18\n",
      "Word \t P(wi|spam)\n",
      "no          0.002568\n",
      "calls       0.000609\n",
      "messages    0.000566\n",
      "missed      0.000131\n",
      "calls       0.000609\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['we', 'have', 'sent', 'jd', 'for', 'customer', 'service', 'cum', 'accounts', 'executive', 'to', 'ur', 'mail', 'id', 'for', 'details', 'contact', 'us']\n",
      "Words found in vocab:\n",
      " ['we', 'have', 'sent', 'jd', 'for', 'customer', 'service', 'cum', 'accounts', 'executive', 'to', 'ur', 'mail', 'id', 'for', 'details', 'contact', 'us']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 1.387e-60\n",
      "Word \t P(wi|ham)\n",
      "we           0.004491\n",
      "have         0.005506\n",
      "sent         0.000769\n",
      "jd           0.000031\n",
      "for          0.006337\n",
      "customer     0.000108\n",
      "service      0.000046\n",
      "cum          0.000108\n",
      "accounts     0.000031\n",
      "executive    0.000031\n",
      "to           0.019594\n",
      "ur           0.003091\n",
      "mail         0.000338\n",
      "id           0.000231\n",
      "for          0.006337\n",
      "details      0.000246\n",
      "contact      0.000154\n",
      "us           0.000754\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 2.950e-58\n",
      "Word \t P(wi|spam)\n",
      "we           0.001741\n",
      "have         0.004875\n",
      "sent         0.000305\n",
      "jd           0.000044\n",
      "for          0.006704\n",
      "customer     0.001741\n",
      "service      0.002002\n",
      "cum          0.000261\n",
      "accounts     0.000044\n",
      "executive    0.000044\n",
      "to           0.023811\n",
      "ur           0.005093\n",
      "mail         0.000044\n",
      "id           0.000174\n",
      "for          0.006704\n",
      "details      0.000348\n",
      "contact      0.002002\n",
      "us           0.000392\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['oh', 'my', 'god', 'i', 've', 'found', 'your', 'number', 'again', 'i', 'm', 'so', 'glad', 'text', 'me', 'back', 'xafter', 'this', 'msgs', 'cst', 'std', 'ntwk', 'chg', '1', '50']\n",
      "Words found in vocab:\n",
      " ['oh', 'my', 'god', 'i', 've', 'found', 'your', 'number', 'again', 'i', 'm', 'so', 'glad', 'text', 'me', 'back', 'this', 'msgs', 'std', 'ntwk', '1', '50']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 1.628e-66\n",
      "Word \t P(wi|ham)\n",
      "oh        0.001400\n",
      "my        0.009197\n",
      "god       0.000584\n",
      "i         0.036942\n",
      "ve        0.001169\n",
      "found     0.000231\n",
      "your      0.005229\n",
      "number    0.000846\n",
      "again     0.000661\n",
      "i         0.036942\n",
      "m         0.005122\n",
      "so        0.005321\n",
      "glad      0.000108\n",
      "text      0.000846\n",
      "me        0.009843\n",
      "back      0.001676\n",
      "this      0.003168\n",
      "msgs      0.000123\n",
      "std       0.000015\n",
      "ntwk      0.000015\n",
      "1         0.000800\n",
      "50        0.000077\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 2.637e-73\n",
      "Word \t P(wi|spam)\n",
      "oh        0.000044\n",
      "my        0.000392\n",
      "god       0.000044\n",
      "i         0.002220\n",
      "ve        0.000392\n",
      "found     0.000044\n",
      "your      0.009228\n",
      "number    0.001175\n",
      "again     0.000131\n",
      "i         0.002220\n",
      "m         0.000740\n",
      "so        0.000958\n",
      "glad      0.000044\n",
      "text      0.004092\n",
      "me        0.001088\n",
      "back      0.000914\n",
      "this      0.003091\n",
      "msgs      0.000305\n",
      "std       0.000305\n",
      "ntwk      0.000087\n",
      "1         0.003961\n",
      "50        0.001785\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['hi', 'babe', 'its', 'chloe', 'how', 'r', 'u', 'i', 'was', 'smashed', 'on', 'saturday', 'night', 'it', 'was', 'great', 'how', 'was', 'your', 'weekend', 'u', 'been', 'missing', 'me', 'sp', 'visionsms', 'com', 'text', 'stop', 'to', 'stop', '150p', 'text']\n",
      "Words found in vocab:\n",
      " ['hi', 'babe', 'its', 'how', 'r', 'u', 'i', 'was', 'smashed', 'on', 'saturday', 'night', 'it', 'was', 'great', 'how', 'was', 'your', 'weekend', 'u', 'been', 'missing', 'me', 'sp', 'visionsms', 'com', 'text', 'stop', 'to', 'stop', '150p', 'text']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 5.839e-95\n",
      "Word \t P(wi|ham)\n",
      "hi           0.001646\n",
      "babe         0.000954\n",
      "its          0.002630\n",
      "how          0.003753\n",
      "r            0.001692\n",
      "u            0.012688\n",
      "i            0.036942\n",
      "was          0.002845\n",
      "smashed      0.000031\n",
      "on           0.004783\n",
      "saturday     0.000138\n",
      "night        0.001292\n",
      "it           0.009043\n",
      "was          0.002845\n",
      "great        0.001307\n",
      "how          0.003753\n",
      "was          0.002845\n",
      "your         0.005229\n",
      "weekend      0.000292\n",
      "u            0.012688\n",
      "been         0.001077\n",
      "missing      0.000308\n",
      "me           0.009843\n",
      "sp           0.000015\n",
      "visionsms    0.000015\n",
      "com          0.000246\n",
      "text         0.000846\n",
      "stop         0.000508\n",
      "to           0.019594\n",
      "stop         0.000508\n",
      "150p         0.000015\n",
      "text         0.000846\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 4.054e-100\n",
      "Word \t P(wi|spam)\n",
      "hi           0.000696\n",
      "babe         0.000348\n",
      "its          0.000174\n",
      "how          0.000305\n",
      "r            0.000958\n",
      "u            0.005659\n",
      "i            0.002220\n",
      "was          0.000261\n",
      "smashed      0.000044\n",
      "on           0.004919\n",
      "saturday     0.000261\n",
      "night        0.000174\n",
      "it           0.001132\n",
      "was          0.000261\n",
      "great        0.000435\n",
      "how          0.000305\n",
      "was          0.000261\n",
      "your         0.009228\n",
      "weekend      0.000218\n",
      "u            0.005659\n",
      "been         0.001567\n",
      "missing      0.000044\n",
      "me           0.001088\n",
      "sp           0.000261\n",
      "visionsms    0.000087\n",
      "com          0.001959\n",
      "text         0.004092\n",
      "stop         0.004484\n",
      "to           0.023811\n",
      "stop         0.004484\n",
      "150p         0.002481\n",
      "text         0.004092\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['0a', 'networks', 'allow', 'companies', 'to', 'bill', 'for', 'sms', 'so', 'they', 'are', 'responsible', 'for', 'their', 'suppliers', 'just', 'as', 'a', 'shop', 'has', 'to', 'give', 'a', 'guarantee', 'on', 'what', 'they', 'sell', 'b', 'g']\n",
      "Words found in vocab:\n",
      " ['networks', 'to', 'bill', 'for', 'sms', 'so', 'they', 'are', 'for', 'their', 'just', 'as', 'a', 'shop', 'has', 'to', 'give', 'a', 'on', 'what', 'they', 'sell', 'b', 'g']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 9.608e-69\n",
      "Word \t P(wi|ham)\n",
      "networks    0.000015\n",
      "to          0.019594\n",
      "bill        0.000108\n",
      "for         0.006337\n",
      "sms         0.000231\n",
      "so          0.005321\n",
      "they        0.001415\n",
      "are         0.005198\n",
      "for         0.006337\n",
      "their       0.000154\n",
      "just        0.003676\n",
      "as          0.001892\n",
      "a           0.013396\n",
      "shop        0.000200\n",
      "has         0.001153\n",
      "to          0.019594\n",
      "give        0.001169\n",
      "a           0.013396\n",
      "on          0.004783\n",
      "what        0.003476\n",
      "they        0.001415\n",
      "sell        0.000154\n",
      "b           0.000984\n",
      "g           0.000261\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 3.200e-72\n",
      "Word \t P(wi|spam)\n",
      "networks    0.000131\n",
      "to          0.023811\n",
      "bill        0.000131\n",
      "for         0.006704\n",
      "sms         0.001393\n",
      "so          0.000958\n",
      "they        0.000566\n",
      "are         0.002829\n",
      "for         0.006704\n",
      "their       0.000044\n",
      "just        0.002786\n",
      "as          0.001088\n",
      "a           0.013407\n",
      "shop        0.000305\n",
      "has         0.001132\n",
      "to          0.023811\n",
      "give        0.000218\n",
      "a           0.013407\n",
      "on          0.004919\n",
      "what        0.000653\n",
      "they        0.000566\n",
      "sell        0.000044\n",
      "b           0.000522\n",
      "g           0.000348\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['rct', 'thnq', 'adrian', 'for', 'u', 'text', 'rgds', 'vatian']\n",
      "Words found in vocab:\n",
      " ['for', 'u', 'text']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 5.886e-08\n",
      "Word \t P(wi|ham)\n",
      "for     0.006337\n",
      "u       0.012688\n",
      "text    0.000846\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 2.089e-08\n",
      "Word \t P(wi|spam)\n",
      "for     0.006704\n",
      "u       0.005659\n",
      "text    0.004092\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['2', '2', '146tf150p']\n",
      "Words found in vocab:\n",
      " ['2', '2']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 1.416e-05\n",
      "Word \t P(wi|ham)\n",
      "2    0.004045\n",
      "2    0.004045\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 7.027e-06\n",
      "Word \t P(wi|spam)\n",
      "2    0.007226\n",
      "2    0.007226\n",
      "Name: spam, dtype: float64\n",
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['hello', 'we', 'need', 'some', 'posh', 'birds', 'and', 'chaps', 'to', 'user', 'trial', 'prods', 'for', 'champneys', 'can', 'i', 'put', 'you', 'down', 'i', 'need', 'your', 'address', 'and', 'dob', 'asap', 'ta', 'r']\n",
      "Words found in vocab:\n",
      " ['hello', 'we', 'need', 'some', 'birds', 'and', 'to', 'user', 'for', 'can', 'i', 'put', 'you', 'down', 'i', 'need', 'your', 'address', 'and', 'asap', 'ta', 'r']\n",
      "\n",
      "p_ham_prior: 8.654e-01\n",
      "p_ham_given_words ~ 2.282e-61\n",
      "Word \t P(wi|ham)\n",
      "hello      0.000569\n",
      "we         0.004491\n",
      "need       0.002107\n",
      "some       0.001400\n",
      "birds      0.000062\n",
      "and        0.010551\n",
      "to         0.019594\n",
      "user       0.000031\n",
      "for        0.006337\n",
      "can        0.005260\n",
      "i          0.036942\n",
      "put        0.000369\n",
      "you        0.024362\n",
      "down       0.000707\n",
      "i          0.036942\n",
      "need       0.002107\n",
      "your       0.005229\n",
      "address    0.000185\n",
      "and        0.010551\n",
      "asap       0.000092\n",
      "ta         0.000092\n",
      "r          0.001692\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "p_spam_prior: 1.346e-01\n",
      "p_spam_given_words ~ 6.041e-71\n",
      "Word \t P(wi|spam)\n",
      "hello      0.000174\n",
      "we         0.001741\n",
      "need       0.000305\n",
      "some       0.000261\n",
      "birds      0.000044\n",
      "and        0.004266\n",
      "to         0.023811\n",
      "user       0.000305\n",
      "for        0.006704\n",
      "can        0.000871\n",
      "i          0.002220\n",
      "put        0.000044\n",
      "you        0.011144\n",
      "down       0.000044\n",
      "i          0.002220\n",
      "need       0.000305\n",
      "your       0.009228\n",
      "address    0.000174\n",
      "and        0.004266\n",
      "asap       0.000261\n",
      "ta         0.000044\n",
      "r          0.000958\n",
      "Name: spam, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114          ham\n",
       "135          ham\n",
       "152         spam\n",
       "159         spam\n",
       "284         spam\n",
       "293    ambigious\n",
       "302         spam\n",
       "319         spam\n",
       "504          ham\n",
       "546          ham\n",
       "741          ham\n",
       "876          ham\n",
       "885          ham\n",
       "953          ham\n",
       "Name: clean_SMS, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_DF.loc[test_DF['Prediction']!=test_DF['Label'],'clean_SMS'].apply(spam_or_ham,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Log Transformation\n",
    "\n",
    "Entry 293, is a long text. Given the probabilities are very small, the longer the text is, the higher the risk for underflow of the values. It wasn't in the original scope of the project but we can solve it by using log transformation to calculate the P(Spam| wi) as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_or_ham_log (txt,debug=False):\n",
    "\n",
    "    import math\n",
    "    \n",
    "    if debug: \n",
    "        print('\\n#--------------------------------------------')\n",
    "        print('SMS:\\n',txt)  \n",
    "    \n",
    "    txt_in_vocab = []\n",
    "    for word in txt:\n",
    "        if word in train_vocab:\n",
    "             txt_in_vocab.append(word)   \n",
    "\n",
    "    if debug: print('Words found in vocab:\\n',txt_in_vocab)\n",
    "\n",
    "    p_ham_given_words  = math.log(p_ham)\n",
    "    p_spam_given_words = math.log(p_spam)   \n",
    "   \n",
    "    if (txt_in_vocab):               \n",
    "        p_ham_given_words  += p_word_DF.loc[\"ham\",txt_in_vocab].apply(np.log).sum()\n",
    "        p_spam_given_words += p_word_DF.loc[\"spam\",txt_in_vocab].apply(np.log).sum()\n",
    "\n",
    "    if(p_ham_given_words>p_spam_given_words):                      \n",
    "        category = \"ham\"\n",
    "    elif (p_ham_given_words<p_spam_given_words):\n",
    "        category = \"spam\"\n",
    "    else:\n",
    "        category = \"ambigious\"\n",
    "    \n",
    "       \n",
    "    if debug:\n",
    "        print()\n",
    "        print('log(p_ham_prior): %.3e' %math.log(p_ham))\n",
    "        print('log(p_ham_given_words) ~ %.3e' %p_ham_given_words)\n",
    "        print('Word','\\t','log(P(wi|ham))')\n",
    "        print(p_word_DF.loc[\"ham\",txt_in_vocab].apply(np.log),'\\n')\n",
    "        print('log(p_spam_prior): %.3e' %math.log(p_spam))\n",
    "        print('log(p_spam_given_words) ~ %.3e' %p_spam_given_words)\n",
    "        print('Word','\\t','log(P(wi|spam))')\n",
    "        print(p_word_DF.loc[\"spam\",txt_in_vocab].apply(np.log))\n",
    "        \n",
    "    return category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check again whether the result looks reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#--------------------------------------------\n",
      "SMS:\n",
      " ['later', 'i', 'guess', 'i', 'needa', 'do', 'mcat', 'study', 'too']\n",
      "Words found in vocab:\n",
      " ['later', 'i', 'guess', 'i', 'do', 'study', 'too']\n",
      "\n",
      "log(p_ham_prior): -1.446e-01\n",
      "log(p_ham_given_words) ~ -4.230e+01\n",
      "Word \t log(P(wi|ham))\n",
      "later   -6.487330\n",
      "i       -3.298393\n",
      "guess   -8.037928\n",
      "i       -3.298393\n",
      "do      -5.326708\n",
      "study   -9.136540\n",
      "too     -6.571591\n",
      "Name: ham, dtype: float64 \n",
      "\n",
      "log(p_spam_prior): -2.006e+00\n",
      "log(p_spam_given_words) ~ -5.862e+01\n",
      "Word \t log(P(wi|spam))\n",
      "later   -10.042075\n",
      "i        -6.110249\n",
      "guess    -8.096165\n",
      "i        -6.110249\n",
      "do       -6.864021\n",
      "study   -10.042075\n",
      "too      -9.348928\n",
      "Name: spam, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_or_ham_log(test_DF.loc[0,'clean_SMS'],debug=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also check whether we could fix the \"ambigious\" issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries: 1114\n",
      "Number of errors: 13\n",
      "Accuracy: 0.9883\n"
     ]
    }
   ],
   "source": [
    "test_DF['Prediction']=test_DF['clean_SMS'].apply(spam_or_ham_log, debug = False)\n",
    "\n",
    "check= (test_DF['Label']==test_DF['Prediction']).value_counts()\n",
    "correct = check[True]\n",
    "error = check[False]\n",
    "accuracy = correct/(correct+error)\n",
    "\n",
    "print(\"Total number of entries:\",correct+error)\n",
    "print(\"Number of errors:\",error)\n",
    "print(\"Accuracy: %.4f\" %accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>clean_SMS</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>spam</td>\n",
       "      <td>Not heard from U4 a while. Call me now am here...</td>\n",
       "      <td>[not, heard, from, u4, a, while, call, me, now...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>spam</td>\n",
       "      <td>More people are dogging in your area now. Call...</td>\n",
       "      <td>[more, people, are, dogging, in, your, area, n...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>ham</td>\n",
       "      <td>Unlimited texts. Limited minutes.</td>\n",
       "      <td>[unlimited, texts, limited, minutes]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>ham</td>\n",
       "      <td>26th OF JULY</td>\n",
       "      <td>[26th, of, july]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nokia phone is lovly..</td>\n",
       "      <td>[nokia, phone, is, lovly]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>ham</td>\n",
       "      <td>No calls..messages..missed calls</td>\n",
       "      <td>[no, calls, messages, missed, calls]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>ham</td>\n",
       "      <td>We have sent JD for Customer Service cum Accou...</td>\n",
       "      <td>[we, have, sent, jd, for, customer, service, c...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>spam</td>\n",
       "      <td>Oh my god! I've found your number again! I'm s...</td>\n",
       "      <td>[oh, my, god, i, ve, found, your, number, agai...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hi babe its Chloe, how r u? I was smashed on s...</td>\n",
       "      <td>[hi, babe, its, chloe, how, r, u, i, was, smas...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>spam</td>\n",
       "      <td>0A$NETWORKS allow companies to bill for SMS, s...</td>\n",
       "      <td>[0a, networks, allow, companies, to, bill, for...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>spam</td>\n",
       "      <td>RCT' THNQ Adrian for U text. Rgds Vatian</td>\n",
       "      <td>[rct, thnq, adrian, for, u, text, rgds, vatian]</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>spam</td>\n",
       "      <td>2/2 146tf150p</td>\n",
       "      <td>[2, 2, 146tf150p]</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hello. We need some posh birds and chaps to us...</td>\n",
       "      <td>[hello, we, need, some, posh, birds, and, chap...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label                                                SMS  \\\n",
       "114  spam  Not heard from U4 a while. Call me now am here...   \n",
       "135  spam  More people are dogging in your area now. Call...   \n",
       "152   ham                  Unlimited texts. Limited minutes.   \n",
       "159   ham                                       26th OF JULY   \n",
       "284   ham                             Nokia phone is lovly..   \n",
       "302   ham                   No calls..messages..missed calls   \n",
       "319   ham  We have sent JD for Customer Service cum Accou...   \n",
       "504  spam  Oh my god! I've found your number again! I'm s...   \n",
       "546  spam  Hi babe its Chloe, how r u? I was smashed on s...   \n",
       "741  spam  0A$NETWORKS allow companies to bill for SMS, s...   \n",
       "876  spam           RCT' THNQ Adrian for U text. Rgds Vatian   \n",
       "885  spam                                      2/2 146tf150p   \n",
       "953  spam  Hello. We need some posh birds and chaps to us...   \n",
       "\n",
       "                                             clean_SMS Prediction  \n",
       "114  [not, heard, from, u4, a, while, call, me, now...        ham  \n",
       "135  [more, people, are, dogging, in, your, area, n...        ham  \n",
       "152               [unlimited, texts, limited, minutes]       spam  \n",
       "159                                   [26th, of, july]       spam  \n",
       "284                          [nokia, phone, is, lovly]       spam  \n",
       "302               [no, calls, messages, missed, calls]       spam  \n",
       "319  [we, have, sent, jd, for, customer, service, c...       spam  \n",
       "504  [oh, my, god, i, ve, found, your, number, agai...        ham  \n",
       "546  [hi, babe, its, chloe, how, r, u, i, was, smas...        ham  \n",
       "741  [0a, networks, allow, companies, to, bill, for...        ham  \n",
       "876    [rct, thnq, adrian, for, u, text, rgds, vatian]        ham  \n",
       "885                                  [2, 2, 146tf150p]        ham  \n",
       "953  [hello, we, need, some, posh, birds, and, chap...        ham  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_DF[test_DF['Prediction']!=test_DF['Label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! The ambigious entry is gone from the error list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of Different Cleaning Methods\n",
    "\n",
    "The following are the results I found while experimenting with different data cleaning methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_errors</th>\n",
       "      <th>Accuracy(%)</th>\n",
       "      <th>Vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>normalized</th>\n",
       "      <td>14</td>\n",
       "      <td>98.83</td>\n",
       "      <td>7783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemmed</th>\n",
       "      <td>15</td>\n",
       "      <td>98.65</td>\n",
       "      <td>6593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keep_capitals</th>\n",
       "      <td>17</td>\n",
       "      <td>98.47</td>\n",
       "      <td>9656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_stop_words</th>\n",
       "      <td>19</td>\n",
       "      <td>98.29</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               N_errors  Accuracy(%)  Vocab_size\n",
       "normalized           14        98.83        7783\n",
       "stemmed              15        98.65        6593\n",
       "keep_capitals        17        98.47        9656\n",
       "no_stop_words        19        98.29        7639"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOMs = pd.DataFrame({\n",
    "                \"N_errors\":[14,17,19,15],\n",
    "                \"Accuracy(%)\":[98.83,98.47,98.29,98.65],\n",
    "                \"Vocab_size\":[7783,9656,7639,6593]\n",
    "             },\n",
    "             index=[\"normalized\",\"keep_capitals\", \"no_stop_words\",\"stemmed\"]\n",
    "            )\n",
    "FOMs.sort_values(by='Accuracy(%)',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming the words and removing the stop words seem to have taken away meaningful data from the dataset and thus caused a reduction in accuracy of the model. They both reduced the size of the vocabulary and potentially the runtime of the model. Since Naive Bayes is a relatively fast algorithm and my dataset is rather small, this figure of merit is less of importance. For a larger dataset stemming might be needed.\n",
    "\n",
    "I was expecting keeping the capitalizations to improve the accuracy since more data would be available but I was wrong. The spam messages had a large amount on capitalized words that was pushing the model to assume any sentence with capitalization to be a spam. On top of that this method increased the vocabulary size and the run time which made this option the least desirable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
